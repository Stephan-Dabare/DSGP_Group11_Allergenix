# -*- coding: utf-8 -*-
"""new.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Zqewijxwn-gQA2bCs6UhEtahpVHjNaiP
"""

!pip install pandas numpy scikit-learn torch transformers tqdm
!pip install fastapi uvicorn nest_asyncio pyngrok

"""**Upload Necessary Files**

"""

from google.colab import files
uploaded = files.upload()

"""**Data Preparation and Label Encoding**"""

import pandas as pd
from sklearn.preprocessing import MultiLabelBinarizer
import pickle

# Load the encoded dataset
final_df = pd.read_csv('final_dataset.csv')
print("Loaded final_dataset.csv successfully.")

# Load MultiLabelBinarizer
with open('mlb.pkl', 'rb') as f:
    mlb = pickle.load(f)
print("Loaded mlb.pkl successfully.")

# Filter out NaN classes
valid_classes = [c for c in mlb.classes_ if pd.notna(c)]
print(f"Valid classes: {valid_classes}")

# Ensure 'combined_text' exists and has no NaNs
if 'combined_text' not in final_df.columns:
    raise KeyError("'combined_text' column not found in final_df.")

final_df = final_df.dropna(subset=['combined_text'])
print(f"Dataset after dropping NaNs in 'combined_text': {len(final_df)} samples.")

# Check for missing classes in the dataset
missing_classes = [c for c in valid_classes if c not in final_df.columns]
if missing_classes:
    print(f"Warning: The following classes are missing in the dataset and will be ignored: {missing_classes}")
    valid_classes = [c for c in valid_classes if c in final_df.columns]

# Select only valid columns for y
y = final_df[valid_classes].values
print(f"Shape of y: {y.shape}")

"""**Splitting the Dataset**"""

from sklearn.model_selection import train_test_split

# Split dataset
X = final_df['combined_text'].values

# y has already been defined above
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

print(f"Training samples: {len(X_train)}")
print(f"Validation samples: {len(X_val)}")
print(f"Test samples: {len(X_test)}")

"""**Defining the Custom Dataset Class and Initializing Tokenizer**"""

import torch
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer

# Define the custom Dataset class
class AllergyDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len=512):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = torch.FloatTensor(self.labels[idx])
        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_len,
            return_token_type_ids=False,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt',
        )
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': label
        }

# Initialize the tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')
print("Tokenizer initialized successfully.")

"""**Creating Dataset Instances and DataLoaders**"""

# Create Dataset instances
train_dataset = AllergyDataset(X_train, y_train, tokenizer)
val_dataset = AllergyDataset(X_val, y_val, tokenizer)
test_dataset = AllergyDataset(X_test, y_test, tokenizer)

print("Dataset instances created successfully.")

# Create DataLoaders
BATCH_SIZE = 16
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)

print("DataLoaders created successfully.")

"""**Model Training**"""

from transformers import BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup
from tqdm import tqdm

# Initialize the BERT model
model = BertForSequenceClassification.from_pretrained(
    'bert-base-multilingual-cased',
    num_labels=len(valid_classes),
    problem_type="multi_label_classification"
)

# Move model to GPU if available
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model.to(device)

print(f"Model loaded on device: {device}")

# Define optimizer, scheduler, and loss function
optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)

epochs = 3
total_steps = len(train_loader) * epochs

scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=0,
    num_training_steps=total_steps
)

loss_fn = torch.nn.BCEWithLogitsLoss().to(device)

def train_epoch(model, data_loader, loss_fn, optimizer, scheduler, device):
    model.train()
    total_loss = 0
    for batch in tqdm(data_loader, desc="Training"):
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        optimizer.zero_grad()
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        loss = loss_fn(logits, labels)
        total_loss += loss.item()

        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        scheduler.step()
    return total_loss / len(data_loader)

def eval_model(model, data_loader, loss_fn, device):
    model.eval()
    total_loss = 0
    preds = []
    true = []
    with torch.no_grad():
        for batch in tqdm(data_loader, desc="Evaluating"):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            logits = outputs.logits
            loss = loss_fn(logits, labels)
            total_loss += loss.item()

            preds.append(torch.sigmoid(logits).cpu())
            true.append(labels.cpu())
    preds = torch.cat(preds).numpy()
    true = torch.cat(true).numpy()
    return total_loss / len(data_loader), preds, true

from sklearn.metrics import f1_score, precision_score, recall_score, classification_report

# Training loop
for epoch in range(epochs):
    print(f'\nEpoch {epoch + 1}/{epochs}')
    print('-' * 10)

    train_loss = train_epoch(
        model,
        train_loader,
        loss_fn,
        optimizer,
        scheduler,
        device
    )
    print(f'Train Loss: {train_loss}')

    val_loss, val_preds, val_true = eval_model(
        model,
        val_loader,
        loss_fn,
        device
    )
    print(f'Validation Loss: {val_loss}')

    # Apply threshold to predictions
    threshold = 0.5
    val_preds_binary = (val_preds >= threshold).astype(int)

    # Calculate metrics
    precision = precision_score(val_true, val_preds_binary, average='micro', zero_division=0)
    recall = recall_score(val_true, val_preds_binary, average='micro', zero_division=0)
    f1 = f1_score(val_true, val_preds_binary, average='micro', zero_division=0)

    print(f'Validation Precision: {precision}')
    print(f'Validation Recall: {recall}')
    print(f'Validation F1 Score: {f1}')

    # Optional: Detailed classification report
    print("\nClassification Report:")
    print(classification_report(val_true, val_preds_binary, target_names=valid_classes, zero_division=0))