# -*- coding: utf-8 -*-
"""BERT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vlmbF4OepjyKybZKK_-TIEKOMlWltHR8
"""

# Save the fine-tuned model
model.save_pretrained('./allergy_bert_model')

# Function to make predictions
def predict_allergy(text):
    encoding = tokenizer(
        text,
        max_length=128,
        padding='max_length',
        truncation=True,
        return_tensors='pt'
    )
    input_ids = encoding['input_ids']
    attention_mask = encoding['attention_mask']
    outputs = model(input_ids, attention_mask=attention_mask)
    preds = torch.argmax(outputs.logits, dim=1).item()
    return label_encoder.inverse_transform([preds])[0]

# Test the prediction function
example_text = "Sugar, wheat flour, vegetable oils, milk powder, cocoa"
predicted_allergy = predict_allergy(example_text)
print("Predicted Allergy:", predicted_allergy)

import pandas as pd
import torch
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from torch.utils.data import Dataset

# Load datasets
df_all_rows = pd.read_csv('/content/all_rows (2).csv')
df_food_data = pd.read_csv('/content/FoodData.csv')

# Clean text (as described earlier)
def clean_text(text):
    if pd.isnull(text):
        return ''
    return text.lower().strip()

df_all_rows['allergens_clean'] = df_all_rows['allergens'].apply(clean_text)
df_food_data['Food_clean'] = df_food_data['Food'].apply(clean_text)

# Merge datasets
merged_data = pd.merge(df_all_rows, df_food_data, left_on='allergens_clean', right_on='Food_clean', how='left')

# Combine text features for model input
merged_data['text'] = (
    merged_data['ingredients_text'].fillna('') + ' ' +
    merged_data['additives_en'].fillna('')
)

# Drop rows with missing Allergy labels after merging
merged_data = merged_data.dropna(subset=['Allergy'])

# Encode the target labels
label_encoder = LabelEncoder()
merged_data['label'] = label_encoder.fit_transform(merged_data['Allergy'])

# Split into train and test sets
train_texts, test_texts, train_labels, test_labels = train_test_split(
    merged_data['text'], merged_data['label'], test_size=0.2, random_state=42
)

from torch.utils.data import Dataset
import torch

class AllergyDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length):
        self.texts = texts  # Expect a list of texts
        self.labels = labels  # Expect a list of labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        # Access list elements using Python indexing
        text = self.texts[idx]
        label = self.labels[idx]

        # Tokenize the text
        encoding = self.tokenizer(
            text,
            max_length=self.max_length,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        )

        # Return the data as a dictionary
        return {
            "input_ids": encoding["input_ids"].squeeze(0),
            "attention_mask": encoding["attention_mask"].squeeze(0),
            "labels": torch.tensor(label, dtype=torch.long)
        }

train_texts = train_texts.tolist()  # Ensure it is a Python list
test_texts = test_texts.tolist()    # Ensure it is a Python list
train_labels = train_labels.tolist()  # Ensure it is a Python list
test_labels = test_labels.tolist()    # Ensure it is a Python list

train_dataset = AllergyDataset(
    texts=train_texts,
    labels=train_labels,
    tokenizer=tokenizer,
    max_length=128
)

eval_dataset = AllergyDataset(
    texts=test_texts,
    labels=test_labels,
    tokenizer=tokenizer,
    max_length=128
)

# Fine-tune the model
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

trainer.train()

# Fine-tune the model
# trainer.train()

# Evaluate the model
results = trainer.evaluate()

# Print evaluation metrics
print("Evaluation Results:", results)

# Sample input
sample_text = "Sugar, wheat flour, milk powder, cocoa butter, soy lecithin"

# Tokenize the input text
sample_tokenized = tokenizer(
    sample_text,
    max_length=128,
    truncation=True,
    padding="max_length",
    return_tensors="pt"
)

from sklearn.preprocessing import MultiLabelBinarizer

# Initialize and fit the MultiLabelBinarizer during training
mlb = MultiLabelBinarizer()
y = mlb.fit_transform(merged_data['Allergy'].str.split(',').tolist())

import pickle

# Save the fitted mlb
with open('mlb.pkl', 'wb') as f:
    pickle.dump(mlb, f)

import numpy as np

# Convert predicted_class into a properly shaped NumPy array
predicted_array = np.zeros((1, len(mlb.classes_)))  # Create an array with the same number of classes
predicted_array[0, predicted_class] = 1  # Set the predicted class to 1 (one-hot encoded)

# Decode the predicted class to allergy type(s)
predicted_allergy = mlb.inverse_transform(predicted_array)
print(f"Predicted Allergy Type: {predicted_allergy}")

import numpy as np
from transformers import BertTokenizer
import torch

# Define the prediction function
def predict_allergy(input_text):
    # Preprocess the input text
    inputs = tokenizer(
        input_text,
        padding=True,
        truncation=True,
        max_length=128,
        return_tensors="pt"
    )

    # Move inputs to the same device as the model
    inputs = {key: val.to(model.device) for key, val in inputs.items()}

    # Get predictions
    model.eval()
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        predicted_class = torch.argmax(logits, dim=1).item()  # Get the class with the highest score

    # Convert predicted_class into a properly shaped NumPy array
    predicted_array = np.zeros((1, len(mlb.classes_)))  # Create an array with the same number of classes
    predicted_array[0, predicted_class] = 1  # Set the predicted class to 1 (one-hot encoded)

    # Decode the predicted class to allergy type(s)
    predicted_allergy = mlb.inverse_transform(predicted_array)
    return predicted_allergy

# Example usage
test_input = "Milk powder, soy lecithin, wheat flour"
predicted_allergy = predict_allergy(test_input)
print(f"Predicted Allergy Type: {predicted_allergy}")

from sklearn.metrics import accuracy_score, hamming_loss, precision_recall_fscore_support
import numpy as np

def evaluate_model_accuracy(model, tokenizer, test_texts, test_labels, mlb):
    """
    Evaluate the model on test data and calculate metrics.
    """
    # Tokenize test texts
    inputs = tokenizer(
        test_texts,  # Ensure this is a list of strings
        padding=True,
        truncation=True,
        max_length=128,
        return_tensors="pt"
    )

    # Move inputs to the model's device
    inputs = {key: val.to(model.device) for key, val in inputs.items()}

    # Predict logits
    model.eval()
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        probabilities = torch.sigmoid(logits).cpu().numpy()  # Convert logits to probabilities

    # Apply threshold to convert probabilities into binary predictions
    threshold = 0.5
    y_pred = (probabilities >= threshold).astype(int)

    # Ensure `test_labels` is in multilabel-indicator format
    if isinstance(test_labels[0], int):  # Convert to multilabel-indicator if needed
        y_test = mlb.transform([[label] for label in test_labels])
    else:
        y_test = np.array(test_labels)

    # Calculate Exact Match Ratio
    exact_match_ratio = accuracy_score(y_test, y_pred)

    # Calculate Precision, Recall, and F1-Score
    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')

    # Calculate Hamming Loss (lower is better)
    hamming = hamming_loss(y_test, y_pred)

    # Output metrics
    metrics = {
        "Exact Match Ratio": exact_match_ratio,
        "Precision": precision,
        "Recall": recall,
        "F1-Score": f1,
        "Hamming Loss": hamming
    }
    return metrics

# Example usage
# Ensure test_labels is a list of lists or a multiclass integer list
metrics = evaluate_model_accuracy(model, tokenizer, test_texts, test_labels, mlb)
print("Model Evaluation Metrics:")
for metric, value in metrics.items():
    print(f"{metric}: {value:.4f}")

# Generate predictions for the test set
all_predictions = []

for text in test_texts:
    # Use the predict_allergy function to predict for each text
    predicted_allergy = predict_allergy(text, model, tokenizer, mlb)
    all_predictions.append(predicted_allergy)

# Convert predictions to the binary format expected by evaluation metrics
all_predictions_binary = mlb.transform(all_predictions)

# Inspect classes learned by MultiLabelBinarizer
print("Classes in MultiLabelBinarizer:", mlb.classes_)

# Inspect unique labels in predictions
unique_predicted_labels = set([tuple(label) for label in all_predictions])
print("Unique Predicted Labels:", unique_predicted_labels)

# Combine old and new classes
updated_classes = set(mlb.classes_).union(set(unique_predicted_labels))

# Refit MultiLabelBinarizer with updated classes
mlb = MultiLabelBinarizer(classes=list(updated_classes))
mlb.fit(updated_classes)

for idx, pred in enumerate(all_predictions):
    print(f"Prediction {idx}: {pred}")
    print(f"Type: {type(pred)}")
    print(f"Shape: {np.shape(pred)}\n")

filtered_predictions = []
for prediction in all_predictions:
    flattened_labels = [label for sublist in prediction for label in sublist]
    filtered_labels = [label for label in flattened_labels if label in mlb.classes_]
    filtered_predictions.append(filtered_labels)

filtered_predictions_binary = mlb.transform(filtered_predictions)

# Convert test_labels to a binary matrix (if not already in that format)
if isinstance(test_labels[0], list):
    test_labels_binary = mlb.transform(test_labels)
else:
    test_labels_binary = np.array(test_labels)

print(f"Shape of test_labels_binary: {test_labels_binary.shape}")
print(f"Shape of filtered_predictions_binary: {filtered_predictions_binary.shape}")

print("Example values from test_labels_binary:")
print(test_labels_binary[:5])

# Convert integer labels into multilabel-indicator format
test_labels_binary_multilabel = mlb.transform([[mlb.classes_[label]] for label in test_labels_binary])

assert test_labels_binary_multilabel.shape == filtered_predictions_binary.shape, "Shape mismatch after conversion!"

print("Classes in MultiLabelBinarizer (mlb):", mlb.classes_)

# Flatten and convert classes to strings
flattened_classes = []
for class_ in mlb.classes_:
    if isinstance(class_, tuple):
        flattened_classes.append(", ".join([str(subclass) for subclass in class_]))
    else:
        flattened_classes.append(class_)

from sklearn.metrics import classification_report

print("Classification Report:")
print(classification_report(test_labels_binary_multilabel, filtered_predictions_binary, target_names=flattened_classes, zero_division=1))

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.metrics import classification_report
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
import torch

# Load Dataset
df = pd.read_csv('/content/all_rows (2).csv')  # Replace with your file path
df['ingredients_text'] = df['ingredients_text'].astype(str)

# Define Allergen Categories
allergen_categories = ['Nut', 'Dairy', 'Gluten', 'Chemical', 'Seafood', 'Soy']  # Extend as needed
mlb = MultiLabelBinarizer(classes=allergen_categories)

# Assign Labels
df['allergen_labels'] = df['allergen_text'].apply(lambda x: x.split(',') if pd.notna(x) else [])
labels = mlb.fit_transform(df['allergen_labels'])

# Tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Tokenize Inputs
max_len = 128
inputs = tokenizer(list(df['ingredients_text']), padding=True, truncation=True, max_length=max_len, return_tensors="pt")

# Split Data
train_texts, val_texts, train_labels, val_labels = train_test_split(
    inputs, labels, test_size=0.2, random_state=42
)

# Prepare Dataset Class
class AllergenDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}, torch.tensor(self.labels[idx])

train_dataset = AllergenDataset(train_texts, train_labels)
val_dataset = AllergenDataset(val_texts, val_labels)

# Load Pre-trained BERT
model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=len(allergen_categories))

# Define Training Arguments
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    num_train_epochs=4,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=10,
    save_total_limit=2
)

# Define Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
    compute_metrics=lambda p: classification_report(
        p.label_ids, p.predictions.argmax(-1), target_names=allergen_categories, output_dict=True
    )
)

# Train Model
trainer.train()

# Evaluate Model
trainer.evaluate()

# Save the Model
model.save_pretrained("./allergen_bert_model")
tokenizer.save_pretrained("./allergen_bert_model")