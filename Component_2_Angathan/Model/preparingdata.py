# -*- coding: utf-8 -*-
"""PreparingData.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c4PEA_53CUp_3ZfDestqba72dXv86RE_
"""

# Install necessary libraries
!pip install pandas numpy scikit-learn torch transformers tqdm

from google.colab import files

# Prompt user to upload files
print("Please upload 'ingredients_data.csv' and 'allergy_classification.csv' files.")
uploaded = files.upload()

import pandas as pd
import pickle

# Load the ingredients data
try:
    ingredients_df = pd.read_csv('/content/all_rows (2) (1).csv')
    print("Loaded 'ingredients_data.csv' successfully.")
    print("\nFirst few rows of 'ingredients_data.csv':")
    print(ingredients_df.head())
except FileNotFoundError:
    print("Error: 'ingredients_data.csv' not found. Please upload the file and try again.")

# Load the allergy classification data
try:
    allergy_classification_df = pd.read_csv('/content/FoodData (1).csv')
    print("\nLoaded 'allergy_classification.csv' successfully.")
    print("\nFirst few rows of 'allergy_classification.csv':")
    print(allergy_classification_df.head())
except FileNotFoundError:
    print("Error: 'allergy_classification.csv' not found. Please upload the file and try again.")

# Check for missing values in 'ingredients_text' and 'additives_en'
print("\nMissing values in 'ingredients_text':", ingredients_df['ingredients_text'].isnull().sum())
print("Missing values in 'additives_en':", ingredients_df['additives_en'].isnull().sum())

# Drop rows with missing 'ingredients_text' or 'additives_en'
ingredients_df = ingredients_df.dropna(subset=['ingredients_text', 'additives_en'])
print(f"Dataset size after dropping missing values: {len(ingredients_df)} samples.")

# Normalize text data by converting to lowercase
def normalize_text(text):
    if isinstance(text, str):
        return text.lower()
    return text

ingredients_df['ingredients_text'] = ingredients_df['ingredients_text'].apply(normalize_text)
ingredients_df['additives_en'] = ingredients_df['additives_en'].apply(normalize_text)
ingredients_df['allergens'] = ingredients_df['allergens'].apply(normalize_text)

print("\nData normalization completed.")

from sklearn.preprocessing import MultiLabelBinarizer

# Extract unique allergies from allergy_classification_df
unique_allergies = allergy_classification_df['Allergy'].unique()
print("\nUnique Allergies:", unique_allergies)

# Split the 'allergens' column into lists
ingredients_df['allergens_list'] = ingredients_df['allergens'].apply(lambda x: [item.strip() for item in x.split(',')] if pd.notna(x) else [])

# Initialize the MultiLabelBinarizer with classes=unique_allergies
mlb = MultiLabelBinarizer(classes=unique_allergies)
mlb.fit(allergy_classification_df['Allergy'])

# Transform the labels
labels = mlb.transform(ingredients_df['allergens_list'])
labels_df = pd.DataFrame(labels, columns=mlb.classes_)

# Display a sample of the encoded labels
print("\nSample of encoded labels:")
print(labels_df.head())

# Combine 'ingredients_text' and 'additives_en' into 'combined_text'
ingredients_df['combined_text'] = ingredients_df['ingredients_text'] + ' ' + ingredients_df['additives_en']

# Verify the combination
print("\nSample of 'combined_text':")
print(ingredients_df['combined_text'].head())

# Create the final dataset by combining 'combined_text' and the encoded labels
final_df = pd.concat([ingredients_df['combined_text'], labels_df], axis=1)

# Shuffle the dataset
final_df = final_df.sample(frac=1, random_state=42).reset_index(drop=True)

# Save the final dataset to CSV
final_df.to_csv('final_dataset.csv', index=False)
print("\nFinal dataset saved as 'final_dataset.csv'.")

# Save the MultiLabelBinarizer using pickle
with open('mlb.pkl', 'wb') as f:
    pickle.dump(mlb, f)
print("MultiLabelBinarizer saved as 'mlb.pkl'.")

