{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T12:22:58.132633Z",
     "start_time": "2025-04-02T12:22:53.443388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "ings = pd.read_csv('data/ingredients_dataset.csv')\n",
    "\n",
    "# Convert all values in 'ingredient' column to strings\n",
    "ings['ingredient'] = ings['ingredient'].astype(str)\n",
    "\n",
    "# Tokenize the ingredients\n",
    "ings['tokenized_ingredients'] = ings['ingredient'].apply(simple_preprocess)\n",
    "\n",
    "# Train Word2Vec on the tokenized ingredients\n",
    "word2vec_model = Word2Vec(\n",
    "    sentences=ings['tokenized_ingredients'],\n",
    "    vector_size=100,       # Dimensionality of the embeddings\n",
    "    window=5,              # Context window size\n",
    "    min_count=1,           # Include all ingredients, even rare ones\n",
    "    workers=4,             # Use 4 CPU cores for training\n",
    "    sg=1,                  # Use Skip-Gram model\n",
    "    epochs=10              # Number of iterations over the corpus\n",
    ")"
   ],
   "id": "49d65534a80a0c20",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Save the model\n",
    "word2vec_model.save(\"data/model/ingredient_word2vec.model\")"
   ],
   "id": "1f79c60566b9c15f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Loading the dataset\n",
    "data = pd.read_csv('data/product_info.csv')\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# The only required columns\n",
    "data = data[['product_id', 'brand_name', 'product_name', 'ingredients', 'primary_category',\n",
    "       'secondary_category', 'tertiary_category']]\n",
    "\n",
    "# Checking for missing values\n",
    "data.isnull().sum()"
   ],
   "id": "99ce76c1b59f548c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Dropping null values\n",
    "data.dropna(subset=['ingredients'], inplace=True)\n",
    "data.dropna(subset=['tertiary_category'], inplace=True)"
   ],
   "id": "f0144d23506312c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Counting duplicates\n",
    "duplicate_count = data.duplicated(keep=False).sum()\n",
    "print(f\"Number of duplicate rows: {duplicate_count}\")"
   ],
   "id": "3e40824e458dd86a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Removing products in the mini size category because it has no tertiary category\n",
    "\n",
    "data.drop(data[data['primary_category'] == \"Mini Size\"].index, inplace=True)"
   ],
   "id": "d60242039573ecd4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Update primary_category where tertiary_category is \"Beauty Supplements\"\n",
    "data.loc[data['tertiary_category'] == \"Beauty Supplements\", 'primary_category'] = \"Other\"\n",
    "# Update primary_category where tertiary_category is \"Holistic Wellness\"\n",
    "data.loc[data['tertiary_category'] == \"Holistic Wellness\", 'primary_category'] = \"Other\"\n",
    "# Update primary_category where tertiary_category is \"Holistic Wellness\"\n",
    "data.loc[data['tertiary_category'] == \"Makeup Removers\", 'primary_category'] = \"Makeup\"\n",
    "# Update primary_category where tertiary_category is \"Teeth Whitening\"\n",
    "data.loc[data['tertiary_category'] == \"Teeth Whitening\", 'primary_category'] = \"Other\"\n",
    "# Update primary_category where tertiary_category is \"Brush Cleaners\"\n",
    "data.loc[data['tertiary_category'] == \"Brush Cleaners\", 'primary_category'] = \"Other\"\n",
    "# Update primary_category where tertiary_category is \"Brush Sets\"\n",
    "data.loc[data['tertiary_category'] == \"Brush Sets\", 'primary_category'] = \"Other\"\n",
    "# Update primary_category where tertiary_category is \"Eye Brushes\"\n",
    "data.loc[data['tertiary_category'] == \"Eye Brushes\", 'primary_category'] = \"Other\"\n",
    "# Update primary_category where tertiary_category is \"Face Brushes\"\n",
    "data.loc[data['tertiary_category'] == \"Face Brushes\", 'primary_category'] = \"Other\"\n",
    "# Update primary_category where tertiary_category is \"Sponges & Applicators\"\n",
    "data.loc[data['tertiary_category'] == \"Sponges & Applicators\", 'primary_category'] = \"Other\"\n",
    "# Update primary_category where tertiary_category is \"Accessories\"\n",
    "data.loc[data['tertiary_category'] == \"Accessories\", 'primary_category'] = \"Other\"\n",
    "# Update primary_category where tertiary_category is \"Hair Supplements\"\n",
    "data.loc[data['tertiary_category'] == \"Hair Supplements\", 'primary_category'] = \"Other\"\n",
    "# Update primary_category where tertiary_category is \"Candles\"\n",
    "data.loc[data['tertiary_category'] == \"Candles\", 'primary_category'] = \"Other\"\n",
    "# Update primary_category where tertiary_category is \"Diffusers\"\n",
    "data.loc[data['tertiary_category'] == \"Diffusers\", 'primary_category'] = \"Other\"\n",
    "# Update primary_category where tertiary_category is \"For Body\"\n",
    "data.loc[data['tertiary_category'] == \"For Body\", 'primary_category'] = \"Skincare\"\n",
    "# Update primary_category where tertiary_category is \"For Face\"\n",
    "data.loc[data['tertiary_category'] == \"For Face\", 'primary_category'] = \"Skincare\"\n",
    "# Update primary_category where tertiary_category is \"Bath & Body\"\n",
    "data.loc[data['primary_category'] == \"Bath & Body\", 'primary_category'] = \"Other\"\n",
    "# Update primary_category where tertiary_category is \"Tools & Brushes\"\n",
    "data.loc[data['primary_category'] == \"Tools & Brushes\", 'primary_category'] = \"Other\"\n",
    "# Update primary_category where tertiary_category is \"Men\"\n",
    "data.loc[data['primary_category'] == \"Men\", 'primary_category'] = \"Other\"\n",
    "# Update primary_category where tertiary_category is \"Body Sunscreen\"\n",
    "data.loc[data['tertiary_category'] == \"Body Sunscreen\", 'primary_category'] = \"Skincare\"\n",
    "# Update primary_category where tertiary_category is \"Blotting Papers\"\n",
    "data.loc[data['tertiary_category'] == \"Blotting Papers\", 'primary_category'] = \"Other\"\n",
    "# Changing the tertiary category name\n",
    "data.loc[data['tertiary_category'] == \"Cologne Gift Sets\", 'tertiary_category'] = \"Cologne\"\n",
    "# Changing the tertiary category name\n",
    "data.loc[data['tertiary_category'] == \"Perfume Gift Sets\", 'tertiary_category'] = \"Perfume\""
   ],
   "id": "4fc92fd769a7202c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Changing the tertiary category name\n",
    "data.loc[data['tertiary_category'] == \"BB & CC Creams\", 'tertiary_category'] = \"BB & CC Cream\""
   ],
   "id": "a966eae69898b839"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Filter the data for the 'Fragrance' primary category\n",
    "fragrance_data = data[data['primary_category'] == 'Fragrance']\n",
    "\n",
    "# Group by tertiary category and count occurrences\n",
    "tertiary_counts = fragrance_data.groupby('tertiary_category').size().reset_index(name='count')\n",
    "\n",
    "# Display all tertiary categories and their counts\n",
    "print(tertiary_counts)"
   ],
   "id": "9a6049ba9fbb738f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Checking for missing values\n",
    "data['primary_category'].isnull().sum()"
   ],
   "id": "3c2d2d56546d3a85"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Checking for value counts of components in primary category\n",
    "data['primary_category'].value_counts()"
   ],
   "id": "f87805b06923866b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Cleaning the Ingredients column",
   "id": "6141a7cdc2d09f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Clean the ingredients column\n",
    "data['ingredients'] = data['ingredients'].str.strip()  # Remove leading and trailing spaces\n",
    "data['ingredients'] = data['ingredients'].str.replace(r'^[^\\w]+|[^\\w]+$', '', regex=True)  # Remove unwanted symbols"
   ],
   "id": "76c57ee2f2f403f7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Attaching detection columns to the dataset",
   "id": "230732543a88290d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import re\n",
    "\n",
    "# Function to normalize ingredient names\n",
    "def normalize_ingredient(ingredient):\n",
    "    ingredient = ingredient.lower()  # Convert to lowercase\n",
    "    ingredient = re.sub(r'[-.,/]+', ' ', ingredient)  # Replace separators with space\n",
    "    ingredient = re.sub(r'\\s+', ' ', ingredient).strip()  # Remove extra spaces\n",
    "    return ingredient\n",
    "\n",
    "\n",
    "# Load ingredient lists\n",
    "with open(\"data/concern_chems.txt\") as f:\n",
    "    concern_chems = [normalize_ingredient(line.strip()) for line in f]\n",
    "\n",
    "with open(\"data/red_list.txt\") as f:\n",
    "    red_list = [normalize_ingredient(line.strip()) for line in f]\n",
    "\n",
    "with open(\"data/the_gens.txt\") as f:\n",
    "    the_gens = [normalize_ingredient(line.strip()) for line in f]\n",
    "\n",
    "# Ensure the ingredients column is in a string format\n",
    "data['ingredients'] = data['ingredients'].astype(str)\n",
    "\n",
    "# Tokenize each ingredient list in the dataset by splitting on commas\n",
    "data['ingredients_list'] = data['ingredients'].apply(\n",
    "    lambda x: [normalize_ingredient(ingredient) for ingredient in x.split(',')])\n",
    "\n",
    "# Create new columns to store detected ingredients from each category\n",
    "data['concerning_chems_detected'] = data['ingredients_list'].apply(\n",
    "    lambda ingredients: [ingredient for ingredient in ingredients if ingredient in concern_chems])\n",
    "data['red_list_chems_detected'] = data['ingredients_list'].apply(\n",
    "    lambda ingredients: [ingredient for ingredient in ingredients if ingredient in red_list])\n",
    "data['allergens_detected'] = data['ingredients_list'].apply(\n",
    "    lambda ingredients: [ingredient for ingredient in ingredients if ingredient in the_gens])\n",
    "\n",
    "# Count detected ingredients for each category\n",
    "data['concerning_chems_count'] = data['concerning_chems_detected'].apply(len)\n",
    "data['red_list_chems_count'] = data['red_list_chems_detected'].apply(len)\n",
    "data['allergens_count'] = data['allergens_detected'].apply(len)\n",
    "\n",
    "# View the results\n",
    "data[['product_name', 'concerning_chems_detected', 'concerning_chems_count', 'red_list_chems_detected',\n",
    "      'red_list_chems_count', 'allergens_detected', 'allergens_count']].head()"
   ],
   "id": "687f585f40b40a54"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load the trained Word2Vec model\n",
    "word2vec_model = Word2Vec.load(\"data/model/ingredient_word2vec.model\")"
   ],
   "id": "28b3ab895702b5f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from gensim.utils import simple_preprocess\n",
    "import json\n",
    "\n",
    "def get_sentence_vector(model, sentence):\n",
    "    \"\"\"\n",
    "    Compute the average vector for a list of words using a Word2Vec model\n",
    "    \"\"\"\n",
    "    vectors = [model.wv[word] for word in sentence if word in model.wv]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)  # Take the average of word vectors\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)  # Return a zero vector if no words match\n",
    "\n",
    "# Process each column to generate embeddings using Word2Vec\n",
    "# Assuming your columns already contain preprocessed tokens or texts\n",
    "\n",
    "# For ingredients_list column\n",
    "data[\"ingredients_embedding\"] = data[\"ingredients_list\"].apply(\n",
    "    lambda x: json.dumps(get_sentence_vector(word2vec_model, x if isinstance(x, list) else simple_preprocess(str(x))).tolist())\n",
    ")\n",
    "\n",
    "# For concern_chems_detected column\n",
    "data[\"concern_chems_embedding\"] = data[\"concerning_chems_detected\"].apply(\n",
    "    lambda x: json.dumps(get_sentence_vector(word2vec_model, x if isinstance(x, list) else simple_preprocess(str(x))).tolist())\n",
    ")\n",
    "\n",
    "# For red_list_detected column\n",
    "data[\"red_list_embedding\"] = data[\"red_list_chems_detected\"].apply(\n",
    "    lambda x: json.dumps(get_sentence_vector(word2vec_model, x if isinstance(x, list) else simple_preprocess(str(x))).tolist())\n",
    ")\n",
    "\n",
    "# For the_gens_detected column\n",
    "data[\"the_gens_embedding\"] = data[\"allergens_detected\"].apply(\n",
    "    lambda x: json.dumps(get_sentence_vector(word2vec_model, x if isinstance(x, list) else simple_preprocess(str(x))).tolist())\n",
    ")"
   ],
   "id": "ceee57e2077af321"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Drop the \"BB & CC Creams\" column from the DataFrame\n",
    "data = data.drop(columns=[\"BB & CC Creams\"])"
   ],
   "id": "272bba9fcfd4e78a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Save the data with embeddings\n",
    "data.to_csv(\"data/new_product_info3.csv\", index=False)"
   ],
   "id": "25904745a2713099"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Extract features (ingredient embeddings) and labels (primary category)\n",
    "X = np.vstack(data['ingredient_embeddings'])\n",
    "y = data['primary_category']\n",
    "\n",
    "# Try different K values\n",
    "k_values = [3, 5, 10, 15]\n",
    "scores = {}\n",
    "\n",
    "for k in k_values:\n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "    # Perform Cross-Validation\n",
    "    cv_scores = cross_val_score(model, X, y, cv=skf, scoring='accuracy')\n",
    "    scores[k] = np.mean(cv_scores)\n",
    "\n",
    "# Find the best K\n",
    "best_k = max(scores, key=scores.get)\n",
    "best_accuracy = scores[best_k]\n",
    "\n",
    "print(f\"Best K: {best_k} with Accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "# Train the final model using the best K\n",
    "final_skf = StratifiedKFold(n_splits=best_k, shuffle=True, random_state=42)\n",
    "final_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Perform final training\n",
    "final_model.fit(X, y)\n",
    "\n",
    "print(\"Final model trained with K =\", best_k)"
   ],
   "id": "988ada1e1676bf45"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import joblib\n",
    "\n",
    "classifier = final_model\n",
    "\n",
    "# Save the model to a file\n",
    "joblib.dump(final_model, \"data/model/new_random_forest.pkl\")"
   ],
   "id": "b654a8a5f7ffe9fc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import joblib\n",
    "\n",
    "# Load the saved model\n",
    "final_model = joblib.load(\"data/model/new_random_forest.pkl\")\n",
    "\n",
    "classifier = final_model"
   ],
   "id": "3a6a35e181bf006b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
