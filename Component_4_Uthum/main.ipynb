{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-12T12:12:16.179230Z",
     "start_time": "2025-01-12T12:12:14.093011Z"
    }
   },
   "source": "from gensim.models import FastText\n",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T12:12:16.195907Z",
     "start_time": "2025-01-12T12:12:16.189474Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gensim\n",
    "print(gensim.__version__)  # Outputs the installed Gensim version\n"
   ],
   "id": "4c8f628b24c82c2a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.3.3\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T04:58:58.021817Z",
     "start_time": "2025-01-16T04:58:57.912102Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('data/new_product_info.csv')"
   ],
   "id": "ebf7567e743744e4",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Predicting Category using Word2Vec model",
   "id": "a2984d9ef383cebc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T12:12:24.718622Z",
     "start_time": "2025-01-12T12:12:16.348985Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Tokenize the ingredients\n",
    "data['tokenized_ingredients'] = data['ingredients'].apply(simple_preprocess)\n",
    "\n",
    "# Train Word2Vec on the tokenized ingredients\n",
    "word2vec_model = Word2Vec(\n",
    "    sentences=data['tokenized_ingredients'],\n",
    "    vector_size=100,       # Dimensionality of the embeddings\n",
    "    window=5,              # Context window size\n",
    "    min_count=1,           # Include all ingredients, even rare ones\n",
    "    workers=4,             # Use 4 CPU cores for training\n",
    "    sg=1,                  # Use Skip-Gram model\n",
    "    epochs=10              # Number of iterations over the corpus\n",
    ")"
   ],
   "id": "ae37e820aeb06e41",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T12:12:25.882323Z",
     "start_time": "2025-01-12T12:12:24.731836Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_sentence_vector(model, sentence):\n",
    "    vectors = [model.wv[word] for word in sentence if word in model.wv]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)  # Take the average of word vectors\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)  # Return a zero vector if no words match\n",
    "\n",
    "# Generate embeddings for each product's ingredients\n",
    "data['ingredient_embeddings'] = data['tokenized_ingredients'].apply(\n",
    "    lambda x: get_sentence_vector(word2vec_model, x)\n",
    ")\n"
   ],
   "id": "ee1bce4756c2d3b0",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T12:12:26.291922Z",
     "start_time": "2025-01-12T12:12:25.896912Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Extract features (ingredient embeddings) and labels (primary category)\n",
    "X = np.vstack(data['ingredient_embeddings'])\n",
    "y = data['primary_category']\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ],
   "id": "177e20e87365b3c4",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T12:12:33.477297Z",
     "start_time": "2025-01-12T12:12:26.311280Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Train a Random Forest classifier\n",
    "classifier = RandomForestClassifier(random_state=42)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = classifier.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ],
   "id": "570b6c1f59044128",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Fragrance       0.99      0.97      0.98       262\n",
      "        Hair       0.95      0.88      0.91       246\n",
      "      Makeup       0.95      0.89      0.92       411\n",
      "    Skincare       0.86      0.96      0.91       451\n",
      "\n",
      "    accuracy                           0.93      1370\n",
      "   macro avg       0.94      0.92      0.93      1370\n",
      "weighted avg       0.93      0.93      0.93      1370\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T12:12:33.511145Z",
     "start_time": "2025-01-12T12:12:33.498439Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example ingredient list for prediction\n",
    "new_ingredients = \"aqua, glycerin, cetyl alcohol, fragrance\"\n",
    "\n",
    "# Tokenize and generate embeddings\n",
    "new_tokens = simple_preprocess(new_ingredients)\n",
    "new_embedding = get_sentence_vector(word2vec_model, new_tokens).reshape(1, -1)\n",
    "\n",
    "# Predict the category\n",
    "predicted_category = classifier.predict(new_embedding)\n",
    "print(\"Predicted Primary Category:\", predicted_category[0])\n"
   ],
   "id": "a502617a5127b1c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Primary Category: Hair\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Predicting Category using Word2Vec model",
   "id": "4948fd744b4dd42c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T12:13:20.175262Z",
     "start_time": "2025-01-12T12:12:33.530083Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "# Train FastText on the tokenized ingredients\n",
    "fasttext_model = FastText(\n",
    "    sentences=data['tokenized_ingredients'],  # Tokenized ingredients\n",
    "    vector_size=100,                          # Dimensionality of word embeddings\n",
    "    window=5,                                 # Context window size\n",
    "    min_count=1,                              # Include all words, even rare ones\n",
    "    workers=4,                                # Use 4 CPU cores for training\n",
    "    sg=1,                                     # Use Skip-Gram (sg=1); CBOW if sg=0\n",
    "    epochs=10                                 # Number of training iterations\n",
    ")\n"
   ],
   "id": "a8864a8240a42211",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T12:13:22.898521Z",
     "start_time": "2025-01-12T12:13:20.216923Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generate embeddings for each product's ingredients\n",
    "data['ingredient_embeddings'] = data['tokenized_ingredients'].apply(\n",
    "    lambda x: get_sentence_vector(fasttext_model, x)\n",
    ")\n",
    "\n",
    "# Extract features (ingredient embeddings) and labels (primary category)\n",
    "X = np.vstack(data['ingredient_embeddings'])\n",
    "y = data['primary_category']\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "id": "9c1f58a13d457abc",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T12:13:37.066593Z",
     "start_time": "2025-01-12T12:13:22.951354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train a Random Forest classifier\n",
    "classifier = RandomForestClassifier(random_state=42)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = classifier.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ],
   "id": "4db421ab82e4b0fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Fragrance       0.98      0.97      0.98       262\n",
      "        Hair       0.96      0.86      0.91       246\n",
      "      Makeup       0.95      0.89      0.92       411\n",
      "    Skincare       0.85      0.96      0.90       451\n",
      "\n",
      "    accuracy                           0.92      1370\n",
      "   macro avg       0.94      0.92      0.93      1370\n",
      "weighted avg       0.93      0.92      0.92      1370\n",
      "\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Predicting Category using BERT",
   "id": "449078975729ce76"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T04:59:25.290947Z",
     "start_time": "2025-01-16T04:59:02.772891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the ingredients column\n",
    "data['tokenized_ingredients'] = data['ingredients'].apply(\n",
    "    lambda x: tokenizer(\n",
    "        text=x,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    ")\n"
   ],
   "id": "9868d212526d2d29",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T05:21:44.389568Z",
     "start_time": "2025-01-16T04:59:27.526230Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import BertModel\n",
    "\n",
    "# Load pre-trained BERT model\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Generate embeddings for each ingredient list\n",
    "def get_bert_embedding(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, padding='max_length', truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Use the [CLS] token embedding\n",
    "    return outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
    "\n",
    "# Apply to the dataset\n",
    "data['bert_embeddings'] = data['ingredients'].apply(\n",
    "    lambda x: get_bert_embedding(x, tokenizer, bert_model)\n",
    ")\n"
   ],
   "id": "2e5c27c2d981bc0f",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T06:04:08.096465Z",
     "start_time": "2025-01-16T06:04:08.023087Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Extract features and labels\n",
    "X = np.vstack(data['bert_embeddings'])\n",
    "y = data['primary_category']\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "id": "bc44c0cd160eafb9",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T06:04:28.559905Z",
     "start_time": "2025-01-16T06:04:11.073319Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Train a Random Forest classifier\n",
    "classifier = RandomForestClassifier(random_state=42)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = classifier.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ],
   "id": "3e1d0ae1a5447379",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Fragrance       0.98      0.96      0.97       262\n",
      "        Hair       0.86      0.53      0.66       246\n",
      "      Makeup       0.85      0.82      0.83       411\n",
      "    Skincare       0.70      0.87      0.78       451\n",
      "\n",
      "    accuracy                           0.81      1370\n",
      "   macro avg       0.85      0.80      0.81      1370\n",
      "weighted avg       0.83      0.81      0.81      1370\n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T06:04:40.474656Z",
     "start_time": "2025-01-16T06:04:40.306686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example ingredient list\n",
    "new_ingredients = \"aqua, glycerin, cetyl alcohol, fragrance\"\n",
    "\n",
    "# Generate embedding for the new ingredient list\n",
    "new_embedding = get_bert_embedding(new_ingredients, tokenizer, bert_model).reshape(1, -1)\n",
    "\n",
    "# Predict the category\n",
    "predicted_category = classifier.predict(new_embedding)\n",
    "print(\"Predicted Primary Category:\", predicted_category[0])"
   ],
   "id": "282e29c40c2fa2bd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Primary Category: Makeup\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T15:13:07.387382Z",
     "start_time": "2025-01-16T15:12:48.094990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "# Convert embeddings to a string (JSON format) before saving\n",
    "data['bert_embeddings'] = data['bert_embeddings'].apply(lambda x: json.dumps(x.tolist()))\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "data.to_csv(\"data/bert_embeddings.csv\", index=False)"
   ],
   "id": "a3af3b630a46ad66",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Loading the bert embeddings\n",
    "data = pd.read_csv(\"data/bert_embeddings.csv\")\n",
    "data['bert_embeddings'] = data['bert_embeddings'].apply(lambda x: np.array(json.loads(x)))\n"
   ],
   "id": "36d13e253510ba4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
