# -*- coding: utf-8 -*-
"""New Dataextract.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yzfzNhNSmVBoSQK887XfMvDkK4KoVZlz
"""

from google.colab import files
files.upload()

from google.colab import drive
drive.mount('/content/drive')

!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json  # Set permissions for security

!mkdir -p /content/data
!kaggle datasets download -d openfoodfacts/world-food-facts -p /content/data --unzip

!mkdir -p /content/data2
!kaggle datasets download boltcutters/food-allergens-and-allergies -p /content/data2 --unzip

import pandas as pd

# Path to the file
file_path = '/content/data/en.openfoodfacts.org.products.tsv'

# Load the TSV file into a DataFrame
df = pd.read_csv(file_path, sep='\t', low_memory=False)

# # Select relevant columns
# columns_to_keep = [
#     'allergens', 'allergens_en', 'ingredients_text'
# ]
# df = df[columns_to_keep]

# # Clean the data
# # Remove rows where allergens or ingredients are missing
# df = df.dropna(subset=['allergens_en', 'ingredients_text'])

# # Fill NaN values in other columns with an empty string
# df = df.fillna("")

# # Save the entire DataFrame as a .txt file
# df.to_csv("cleaned_world_food_facts.csv", sep='\t', index=False)

# print("Data cleaned and saved as cleaned_world_food_facts.csv")

# Display the first few rows of the cleaned dataset
df.tail()

#display columns and allergens_en ingredients_text

# Select relevant columns
columns_to_keep = ['allergens', 'ingredients_text','additives_en']
df2 = df[columns_to_keep]

# Display the first few rows
df2.tail()

# Count missing values in 'allergens'
missing_allergens = df2['allergens'].isnull().sum()
print(f"Number of missing values in 'allergens': {missing_allergens}")

# Display available values in 'allergens'
available_allergens = df2['allergens'].dropna().unique()
print("\nAvailable values in 'allergens':")
print(available_allergens)

# Print the total count of available values
total_available_allergens = len(available_allergens)
print(f"\nTotal number of available unique allergens: {total_available_allergens}")

# import pandas as pd
# import numpy as np
# import re
# from sklearn.model_selection import train_test_split
# from sklearn.preprocessing import MultiLabelBinarizer
# from sklearn.ensemble import RandomForestClassifier
# from sklearn.multiclass import OneVsRestClassifier
# from sklearn.metrics import classification_report
# from gensim.models import Word2Vec
# from nltk.tokenize import word_tokenize

# # Load dataset
# df_datasample = pd.read_csv('/content/first_100_rows.csv')

# # Step 1: Normalize allergen labels
# def normalize_allergens(allergens):
#     allergens = allergens.lower()  # Convert to lowercase
#     allergens = re.sub(r'\s+', '_', allergens)  # Replace spaces with underscores
#     return allergens

# df_datasample['allergens'] = df_datasample['allergens'].apply(lambda x: [normalize_allergens(allergen.strip()) for allergen in x.split(',')])

# # Step 2: Preprocess ingredients_text and additives_en columns
# # Clean and tokenize the text
# df_datasample['ingredients_text'] = df_datasample['ingredients_text'].apply(lambda x: re.sub(r'[^a-zA-Z\s]', '', x.lower()))
# df_datasample['ingredients_text'] = df_datasample['ingredients_text'].apply(word_tokenize)

# df_datasample['additives_en'] = df_datasample['additives_en'].apply(lambda x: re.sub(r'[^a-zA-Z\s]', '', str(x).lower()))  # Ensure string
# df_datasample['additives_en'] = df_datasample['additives_en'].apply(word_tokenize)

# # Step 3: Combine ingredients and additives for Word2Vec training
# df_datasample['combined_text'] = df_datasample['ingredients_text'] + df_datasample['additives_en']

# # Train a Word2Vec model on the combined text
# sentences = df_datasample['combined_text'].tolist()
# word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4, seed=42)

# # Step 4: Generate vector representation for each combined text (ingredients + additives)
# def get_sentence_vector(tokens, model):
#     # Get vectors for words in the sentence, then average them
#     word_vectors = [model.wv[word] for word in tokens if word in model.wv]
#     if len(word_vectors) == 0:  # Handle case where no words are in the model's vocab
#         return np.zeros(model.vector_size)
#     sentence_vector = np.mean(word_vectors, axis=0)
#     return sentence_vector

# df_datasample['vectorized_text'] = df_datasample['combined_text'].apply(lambda x: get_sentence_vector(x, word2vec_model))

# # Step 5: Prepare data for model training
# X = np.vstack(df_datasample['vectorized_text'].values)
# mlb = MultiLabelBinarizer()
# y = mlb.fit_transform(df_datasample['allergens'])

# # Step 6: Split the data
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# # Step 7: Train the model with Random Forest
# model = OneVsRestClassifier(RandomForestClassifier(n_estimators=100, random_state=42))
# model.fit(X_train, y_train)

# # Step 8: Predict and evaluate
# y_pred = model.predict(X_test)
# print(classification_report(y_test, y_pred, target_names=mlb.classes_, zero_division=0))

# # Function to predict allergens for new ingredients
# def predict_allergens(ingredients_text, additives_text):
#     # Preprocess and vectorize input text
#     ingredients_text = re.sub(r'[^a-zA-Z\s]', '', ingredients_text.lower())
#     additives_text = re.sub(r'[^a-zA-Z\s]', '', additives_text.lower())

#     tokens = word_tokenize(ingredients_text) + word_tokenize(additives_text)
#     vectorized_text = get_sentence_vector(tokens, word2vec_model).reshape(1, -1)

#     # Predict allergens
#     predicted_labels = model.predict(vectorized_text)
#     return mlb.inverse_transform(predicted_labels)

# # Test the function with an example
# example_ingredients = "Sugar, wheat flour, vegetable oils, milk powder, cocoa"
# example_additives = "E322, lecithin, soy"

# predicted_allergens = predict_allergens(example_ingredients, example_additives)
# print("Predicted Allergens:", predicted_allergens)


df_datasample = df2

df_datasample.tail()

# Remove rows with NaN values in specified columns
df_datasample = df_datasample.dropna(subset=['allergens', 'ingredients_text', 'additives_en'])

df_datasample.tail()

# Save the entire DataFrame to a CSV file
df_datasample.to_csv('/content/all_rows.csv', index=False)

df_datasample= pd.read_csv('/content/all_rows.csv')

pip install deep-translator

from deep_translator import GoogleTranslator

# Function to translate text using deep-translator
def translate_text(text):
    try:
        # Using GoogleTranslator to translate from French to English
        translation = GoogleTranslator(source='fr', target='en').translate(text)
        return translation
    except Exception as e:
        print(f"Error translating: {e}")
        return text  # Return the original text if translation fails

# Apply translation to all rows in the specified columns without row limitations
df_datasample['allergens'] = df_datasample['allergens'].apply(translate_text)
df_datasample['ingredients_text'] = df_datasample['ingredients_text'].apply(translate_text)
df_datasample['additives_en'] = df_datasample['additives_en'].apply(translate_text)

# Save the fully translated DataFrame to a CSV file
df_datasample.to_csv('/content/translated_all_rows.csv', index=False)

print("Translation completed for all rows and saved to translated_all_rows.csv")

df_datasample

# save first 100 rows of df_datasample into a csv file

# Save the first 100 rows to a CSV file
df_datasample.head(100).to_csv('/content/first_100_rows.csv', index=False)

"""TF IDF method"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.multiclass import OneVsRestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from sklearn.preprocessing import MultiLabelBinarizer

newdata = pd.read_csv('/content/first_100_rows.csv')

# Step 1: Convert allergens column to a list of labels
newdata['allergens'] = newdata['allergens'].apply(lambda x: [allergen.strip() for allergen in x.split(',')])

# Step 2: Binarize the target labels
mlb = MultiLabelBinarizer()
y = mlb.fit_transform(newdata['allergens'])

# Step 3: Split the data
X_train, X_test, y_train, y_test = train_test_split(newdata['ingredients_text'], y, test_size=0.2, random_state=42)

# Step 4: TF-IDF Vectorization
tfidf = TfidfVectorizer(max_features=5000, stop_words='english')
X_train_tfidf = tfidf.fit_transform(X_train)
X_test_tfidf = tfidf.transform(X_test)

# Step 5: Train the model with Logistic Regression
model = OneVsRestClassifier(LogisticRegression(max_iter=1000, random_state=42))
model.fit(X_train_tfidf, y_train)

# Step 6: Predict and evaluate
y_pred = model.predict(X_test_tfidf)
print(classification_report(y_test, y_pred, target_names=mlb.classes_))

# Function to predict allergens for new ingredients
def predict_allergens(ingredients_text):
    ingredients_tfidf = tfidf.transform([ingredients_text])
    predicted_labels = model.predict(ingredients_tfidf)
    return mlb.inverse_transform(predicted_labels)

# Test the function with an example
example_ingredients = "Wheyproteinconcentrate       "
predicted_allergens = predict_allergens(example_ingredients)
print("Predicted Allergens:", predicted_allergens)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.multiclass import OneVsRestClassifier
from sklearn.metrics import classification_report
import re

# Sample DataFrame (replace this with actual data)
newdf_datasample = pd.read_csv('/content/first_100_rows.csv')

# Normalize allergen labels
def normalize_allergens(allergens):
    allergens = allergens.lower()  # Convert to lowercase
    allergens = re.sub(r'\s+', '_', allergens)  # Replace spaces with underscores
    return allergens

newdf_datasample['allergens'] = newdf_datasample['allergens'].apply(lambda x: [normalize_allergens(allergen.strip()) for allergen in x.split(',')])

# Binarize the target labels
mlb = MultiLabelBinarizer()
y = mlb.fit_transform(newdf_datasample['allergens'])

# Split the data
X_train, X_test, y_train, y_test = train_test_split(newdf_datasample['ingredients_text'], y, test_size=0.2, random_state=42)

# TF-IDF Vectorization
tfidf = TfidfVectorizer(max_features=5000, stop_words='english')
X_train_tfidf = tfidf.fit_transform(X_train)
X_test_tfidf = tfidf.transform(X_test)

# Train the model with Random Forest
model = OneVsRestClassifier(RandomForestClassifier(n_estimators=100, random_state=42))
model.fit(X_train_tfidf, y_train)

# Predict and evaluate
y_pred = model.predict(X_test_tfidf)
print(classification_report(y_test, y_pred, target_names=mlb.classes_, zero_division=0))

# Function to predict allergens for new ingredients
def predict_allergens(ingredients_text):
    ingredients_tfidf = tfidf.transform([ingredients_text])
    predicted_labels = model.predict(ingredients_tfidf)
    return mlb.inverse_transform(predicted_labels)

# Test the function with an example
example_ingredients = "Sugar, wheat flour, vegetable oils, milk powder, cocoa"
predicted_allergens = predict_allergens(example_ingredients)
print("Predicted Allergens:", predicted_allergens)

# Test the function with an example
example_ingredients = "Vollmilchpulver, Vollmilchpulver, Soya, Butterreinfett, Haselnusskerne, Soya, Mandeln, Pisatazien"
predicted_allergens = predict_allergens(example_ingredients)
print("Predicted Allergens:", predicted_allergens)

"""word 2 vec transformer"""

import nltk
nltk.download('punkt')

import pandas as pd
import numpy as np
import re
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.multiclass import OneVsRestClassifier
from sklearn.metrics import classification_report
from gensim.models import Word2Vec
from nltk.tokenize import word_tokenize

# Load dataset
df_datasample = pd.read_csv('/content/first_100_rows.csv')

# Step 1: Normalize allergen labels
def normalize_allergens(allergens):
    allergens = allergens.lower()  # Convert to lowercase
    allergens = re.sub(r'\s+', '_', allergens)  # Replace spaces with underscores
    return allergens

df_datasample['allergens'] = df_datasample['allergens'].apply(lambda x: [normalize_allergens(allergen.strip()) for allergen in x.split(',')])

# Step 2: Preprocess ingredients_text and additives_en columns
# Clean and tokenize the text
df_datasample['ingredients_text'] = df_datasample['ingredients_text'].apply(lambda x: re.sub(r'[^a-zA-Z\s]', '', x.lower()))
df_datasample['ingredients_text'] = df_datasample['ingredients_text'].apply(word_tokenize)

df_datasample['additives_en'] = df_datasample['additives_en'].apply(lambda x: re.sub(r'[^a-zA-Z\s]', '', str(x).lower()))  # Ensure string
df_datasample['additives_en'] = df_datasample['additives_en'].apply(word_tokenize)

# Step 3: Combine ingredients and additives for Word2Vec training
df_datasample['combined_text'] = df_datasample['ingredients_text'] + df_datasample['additives_en']

# Train a Word2Vec model on the combined text
sentences = df_datasample['combined_text'].tolist()
word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4, seed=42)

# Step 4: Generate vector representation for each combined text (ingredients + additives)
def get_sentence_vector(tokens, model):
    # Get vectors for words in the sentence, then average them
    word_vectors = [model.wv[word] for word in tokens if word in model.wv]
    if len(word_vectors) == 0:  # Handle case where no words are in the model's vocab
        return np.zeros(model.vector_size)
    sentence_vector = np.mean(word_vectors, axis=0)
    return sentence_vector

df_datasample['vectorized_text'] = df_datasample['combined_text'].apply(lambda x: get_sentence_vector(x, word2vec_model))

# Step 5: Prepare data for model training
X = np.vstack(df_datasample['vectorized_text'].values)
mlb = MultiLabelBinarizer()
y = mlb.fit_transform(df_datasample['allergens'])

# Step 6: Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 7: Train the model with Random Forest
model = OneVsRestClassifier(RandomForestClassifier(n_estimators=100, random_state=42))
model.fit(X_train, y_train)

# Step 8: Predict and evaluate
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred, target_names=mlb.classes_, zero_division=0))

# Function to predict allergens for new ingredients
def predict_allergens(ingredients_text, additives_text):
    # Preprocess and vectorize input text
    ingredients_text = re.sub(r'[^a-zA-Z\s]', '', ingredients_text.lower())
    additives_text = re.sub(r'[^a-zA-Z\s]', '', additives_text.lower())

    tokens = word_tokenize(ingredients_text) + word_tokenize(additives_text)
    vectorized_text = get_sentence_vector(tokens, word2vec_model).reshape(1, -1)

    # Predict allergens
    predicted_labels = model.predict(vectorized_text)
    return mlb.inverse_transform(predicted_labels)

# Test the function with an example
example_ingredients = "Sugar, wheat flour, vegetable oils, milk powder, cocoa"
example_additives = "E322, lecithin, soy"

predicted_allergens = predict_allergens(example_ingredients, example_additives)
print("Predicted Allergens:", predicted_allergens)

# Sample ingredient and additive lists to test the model
sample_predictions = [
    {
        "ingredients_text": "Sugar, wheat flour, vegetable oils, milk powder, cocoa",
        "additives_en": "E322, lecithin, soy"
    },
    {
        "ingredients_text": "Cane sugar, almond flour, natural flavors, salt",
        "additives_en": "E306, tocopherols, sunflower oil"
    },
    {
        "ingredients_text": "Whole grain oats, brown sugar, wheat flour, milk",
        "additives_en": "E202, potassium sorbate, preservative"
    },
    {
        "ingredients_text": "Enriched flour (wheat flour, niacin), eggs, salt, canola oil",
        "additives_en": "E300, ascorbic acid, antioxidant"
    },
    {
        "ingredients_text": "Peanut butter, sugar, hydrogenated vegetable oil",
        "additives_en": "E471, emulsifier"
    },
    {
        "ingredients_text": "Dried apricots, walnuts, honey, lemon juice",
        "additives_en": "E220, sulfur dioxide"
    },
    {
        "ingredients_text": "Milk chocolate (sugar, cocoa butter, milk powder), barley malt extract",
        "additives_en": "E322, lecithin"
    },
    {
        "ingredients_text": "Corn syrup, palm oil, pecans, butter",
        "additives_en": "E330, citric acid"
    },
    {
        "ingredients_text": "Red wine vinegar, water, mustard seed, garlic",
        "additives_en": "E224, potassium metabisulfite"
    },
    {
        "ingredients_text": "Cheddar cheese, milk, salt, enzymes",
        "additives_en": "E235, natamycin, preservative"
    }
]

# Generate predictions for each sample
for sample in sample_predictions:
    ingredients_text = sample["ingredients_text"]
    additives_en = sample["additives_en"]

    predicted_allergens = predict_allergens(ingredients_text, additives_en)
    print(f"Ingredients: {ingredients_text}")
    print(f"Additives: {additives_en}")
    print(f"Predicted Allergens: {predicted_allergens}")
    print("-" * 50)

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import numpy as np

# Extract Word2Vec vectors for each ingredient list
# Ensure 'vectorized_text' column is in your DataFrame from previous code
X = np.vstack(df_datasample['vectorized_text'].values)
y_labels = df_datasample['allergens'].apply(lambda x: ', '.join(x))  # Concatenate allergens for labeling

# 1. PCA Visualization
def plot_pca(X, y_labels):
    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(X)

    plt.figure(figsize=(10, 8))
    sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=y_labels, palette="tab10", legend=False)
    plt.title("PCA of Ingredient Texts")
    plt.xlabel("PCA Component 1")
    plt.ylabel("PCA Component 2")
    plt.show()

# 2. t-SNE Visualization
def plot_tsne(X, y_labels, perplexity=30):
    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)
    X_tsne = tsne.fit_transform(X)

    plt.figure(figsize=(10, 8))
    sns.scatterplot(x=X_tsne[:, 0], y=X_tsne[:, 1], hue=y_labels, palette="tab10", legend=False)
    plt.title("t-SNE of Ingredient Texts")
    plt.xlabel("t-SNE Component 1")
    plt.ylabel("t-SNE Component 2")
    plt.show()

# Run PCA and t-SNE plots
plot_pca(X, y_labels)
plot_tsne(X, y_labels)

"""# regex to get allerginicty"""

pip install PyPDF2

from PyPDF2 import PdfReader
import re
import pandas as pd

# Path to the PDF file
pdf_path = '/content/sample_data/AllergenOnlineV22.pdf'

# Initialize PDF reader and lists for allergens and allergenicity values
reader = PdfReader(pdf_path)
allergens_list = []
allergenicity_list = []

# Parse the PDF to extract Allergen and Allergenicity information
for page in reader.pages:
    text = page.extract_text()
    lines = text.split('\n')
    for line in lines:
        # Regex to capture allergen and allergenicity info based on common format in the PDF
        match = re.search(r'(.+?)\s+\bIgE\b\s+([\w\s]+)', line)
        if match:
            allergen = match.group(1).strip()  # Extract allergen name
            allergenicity = match.group(2).strip()  # Extract allergenicity value
            allergens_list.append(allergen)
            allergenicity_list.append(allergenicity)

# Create a DataFrame with extracted allergen and allergenicity data
df_allergenicity = pd.DataFrame({
    'Allergen': allergens_list,
    'Allergenicity': allergenicity_list
})

# Save the DataFrame to a CSV file
df_allergenicity.to_csv('/content/extracted_allergenicity_data.csv', index=False)

df_allergenicity.head()

"""# new data with allergy types"""

alergytypedata = pd.read_csv('/content/data2/FoodData.csv')

alergytypedata.head()

pip install fuzzywuzzy python-Levenshtein

import pandas as pd
from fuzzywuzzy import fuzz
from fuzzywuzzy import process

# Load both datasets
df_sample = pd.read_csv('/content/first_100_rows.csv')
alergytypedata = pd.read_csv('/content/data2/FoodData.csv')

# Normalize allergen names in both datasets
df_sample['allergens'] = df_sample['allergens'].str.lower()
alergytypedata['Food'] = alergytypedata['Food'].str.lower()

# Function to get the best fuzzy match for each allergen in df_sample from the allergy type dataset
def get_closest_match(allergen, choices, threshold=80):
    match, score = process.extractOne(allergen, choices, scorer=fuzz.token_sort_ratio)
    return match if score >= threshold else None

# Expand allergens column to match individually
df_sample['allergen_list'] = df_sample['allergens'].apply(lambda x: x.split(','))
df_sample_expanded = df_sample.explode('allergen_list')
df_sample_expanded['allergen_list'] = df_sample_expanded['allergen_list'].str.strip()

# Apply fuzzy matching
allergy_type_map = {}
for allergen in df_sample_expanded['allergen_list'].unique():
    closest_match = get_closest_match(allergen, alergytypedata['Food'].unique())
    if closest_match:
        allergy_type_map[allergen] = closest_match

# Map allergens to closest matches from allergy type dataset
df_sample_expanded['matched_food'] = df_sample_expanded['allergen_list'].map(allergy_type_map)
df_merged = pd.merge(df_sample_expanded, alergytypedata[['Food', 'Allergy']],
                     left_on='matched_food', right_on='Food', how='left')

# Aggregate allergy types and clean up columns
df_merged['AllergyType'] = df_merged.groupby(level=0)['Allergy'].transform(lambda x: ', '.join(filter(pd.notna, x)))
df_final = df_merged.drop(['allergen_list', 'matched_food', 'Food', 'Allergy'], axis=1).drop_duplicates()

# Save to CSV and display
df_final.to_csv('/content/updated_dataset_with_fuzzy_allergytype.csv', index=False)
print("Merged dataset saved with fuzzy-matched AllergyType column.")

df_final.head()

"""# random forest TFIDF for allergy type"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.metrics import classification_report

# Load the merged dataset with allergens and allergy types
df = pd.read_csv('/content/updated_dataset_with_fuzzy_allergytype.csv')

# Preprocess the allergens and allergy types
# Assume that `allergens` contains allergen names as a string, and `AllergyType` contains types as comma-separated values
df['allergens'] = df['allergens'].fillna('')
df['AllergyType'] = df['AllergyType'].fillna('')

# Vectorize allergens with TF-IDF
tfidf = TfidfVectorizer()
X = tfidf.fit_transform(df['allergens'])

# MultiLabel Binarizer for allergy types
mlb = MultiLabelBinarizer()
y = mlb.fit_transform(df['AllergyType'].str.split(','))

# Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a model (Random Forest)
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Predict on test set and evaluate
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred, target_names=mlb.classes_))

# Function to predict allergy types for new allergens
def predict_allergy_type(allergen_list):
    # Transform the input using the fitted TF-IDF vectorizer
    allergen_text = ' '.join(allergen_list)
    X_input = tfidf.transform([allergen_text])

    # Predict allergy types
    predicted_labels = model.predict(X_input)
    return mlb.inverse_transform(predicted_labels)

# Test the function with an example
example_allergens = ["wheat", "peanut"]
predicted_allergy_types = predict_allergy_type(example_allergens)
print("Predicted Allergy Types:", predicted_allergy_types)

# Let's create a few sample examples to test the prediction function with various allergens.
sample_allergens = [
    ["peanut", "soy", "milk"],
    ["wheat", "egg"],
    ["fish", "shellfish", "crustacean"],
    ["tree nuts", "almond", "hazelnut"],
    ["gluten", "dairy", "soy"],
]

# Using the predict_allergy_type function to generate predictions for each sample allergen list
sample_predictions = {tuple(allergens): predict_allergy_type(allergens) for allergens in sample_allergens}
sample_predictions

"""# word2vec"""

import pandas as pd
import numpy as np
from gensim.models import Word2Vec
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.metrics import classification_report
from nltk.tokenize import word_tokenize

# Load the merged dataset
df = pd.read_csv('/content/updated_dataset_with_fuzzy_allergytype.csv')

# Preprocess the allergens and allergy types
df['allergens'] = df['allergens'].fillna('')
df['AllergyType'] = df['AllergyType'].fillna('')

# Tokenize allergens
df['allergens'] = df['allergens'].apply(lambda x: x.lower().split(','))

# Step 1: Train Word2Vec model on allergens data
sentences = df['allergens'].tolist()
word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4, seed=42)

# Step 2: Generate sentence vectors by averaging word vectors
def get_sentence_vector(tokens, model):
    word_vectors = [model.wv[word] for word in tokens if word in model.wv]
    if len(word_vectors) == 0:
        return np.zeros(model.vector_size)
    return np.mean(word_vectors, axis=0)

df['vectorized_text'] = df['allergens'].apply(lambda x: get_sentence_vector(x, word2vec_model))

# Step 3: Prepare data for training
X = np.vstack(df['vectorized_text'].values)
mlb = MultiLabelBinarizer()
y = mlb.fit_transform(df['AllergyType'].str.split(','))

# Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a Random Forest Classifier
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Predict on test set and evaluate
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred, target_names=mlb.classes_))

# Prediction function using Word2Vec
def predict_allergy_type(allergen_list):
    tokens = [word.lower() for word in allergen_list]
    vectorized_text = get_sentence_vector(tokens, word2vec_model).reshape(1, -1)
    predicted_labels = model.predict(vectorized_text)
    return mlb.inverse_transform(predicted_labels)

# Test the function with sample allergen lists
sample_allergens = [
    ["peanut", "soy", "milk"],
    ["wheat", "egg"],
    ["fish", "shellfish", "crustacean"],
    ["tree nuts", "almond", "hazelnut"],
    ["gluten", "dairy", "soy"],
]

# Generate predictions for sample allergens
sample_predictions = {tuple(allergens): predict_allergy_type(allergens) for allergens in sample_allergens}
sample_predictions

"""## with ingredients"""

import pandas as pd
import numpy as np
from gensim.models import Word2Vec
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.metrics import classification_report
from nltk.tokenize import word_tokenize
import re

# Load the dataset
df = pd.read_csv('/content/updated_dataset_with_fuzzy_allergytype.csv')

# Preprocess the ingredients and allergy types
df['ingredients_text'] = df['ingredients_text'].fillna('')
df['AllergyType'] = df['AllergyType'].fillna('')

# Tokenize ingredients_text and clean the text
def preprocess_ingredients(text):
    text = re.sub(r'[^a-zA-Z\s]', '', text.lower())  # Remove punctuation and lowercase
    tokens = word_tokenize(text)
    return tokens

df['ingredients_text'] = df['ingredients_text'].apply(preprocess_ingredients)

# Step 1: Train Word2Vec model on ingredients data
sentences = df['ingredients_text'].tolist()
word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4, seed=42)

# Step 2: Generate sentence vectors by averaging word vectors
def get_sentence_vector(tokens, model):
    word_vectors = [model.wv[word] for word in tokens if word in model.wv]
    if len(word_vectors) == 0:
        return np.zeros(model.vector_size)
    return np.mean(word_vectors, axis=0)

df['vectorized_text'] = df['ingredients_text'].apply(lambda x: get_sentence_vector(x, word2vec_model))

# Step 3: Prepare data for training
X = np.vstack(df['vectorized_text'].values)
mlb = MultiLabelBinarizer()
y = mlb.fit_transform(df['AllergyType'].str.split(','))

# Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a Random Forest Classifier
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Predict on test set and evaluate
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred, target_names=mlb.classes_))

# Prediction function using Word2Vec
def predict_allergy_type(ingredient_list):
    tokens = preprocess_ingredients(' '.join(ingredient_list))
    vectorized_text = get_sentence_vector(tokens, word2vec_model).reshape(1, -1)
    predicted_labels = model.predict(vectorized_text)
    return mlb.inverse_transform(predicted_labels)

# Test the function with sample ingredient lists
sample_ingredients = [
    ["sugar", "wheat flour", "vegetable oils", "milk powder", "cocoa"],
    ["cane sugar", "almond flour", "natural flavors", "salt"],
    ["whole grain oats", "brown sugar", "wheat flour", "milk"],
    ["peanut butter", "sugar", "hydrogenated vegetable oil"],
    ["dried apricots", "walnuts", "honey", "lemon juice"]
]

# Generate predictions for sample ingredients
sample_predictions = {tuple(ingredients): predict_allergy_type(ingredients) for ingredients in sample_ingredients}
sample_predictions

from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset
import torch
import pandas as pd

# Load dataset
df = pd.read_csv('/content/all_rows.csv')  # Replace with dataset
df['label'] = df['label'].astype(int)  # Ensure labels are integers

# Split data
train_texts, val_texts, train_labels, val_labels = train_test_split(
    df['ingredients_text'].tolist(), df['label'].tolist(), test_size=0.2, random_state=42
)

# Define Dataset class
class FoodDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]
        inputs = self.tokenizer(
            text,
            max_length=self.max_len,
            padding="max_length",
            truncation=True,
            return_tensors="pt",
        )
        return {
            "input_ids": inputs["input_ids"].squeeze(0),
            "attention_mask": inputs["attention_mask"].squeeze(0),
            "labels": torch.tensor(label, dtype=torch.long),
        }

# Load tokenizer and model
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)

# Prepare datasets
train_dataset = FoodDataset(train_texts, train_labels, tokenizer)
val_dataset = FoodDataset(val_texts, val_labels, tokenizer)

# Training arguments
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=4,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=10,
    save_steps=10,
)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
)

# Train the model
trainer.train()

# Evaluate the model
trainer.evaluate()